{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "2e70d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, adjusted_rand_score\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.stats import kurtosis\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Models\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# Tools\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f9745f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random Seed\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "np.random.seed(888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "64c390f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The two datasets that will be used to train the models in this project. Also used in project 1.\n",
    "'''\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "iris = datasets.load_iris()\n",
    "breast_cancer_X, breast_cancer_Y = breast_cancer.data, breast_cancer.target\n",
    "iris_X, iris_Y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "b505c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part 1: Clustering Algorithms\n",
    "'''\n",
    "def gaussian_clustering(X, y, data, title, savefig=True):\n",
    "    n_components = len(np.unique(y))\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm.means_init = np.array(\n",
    "        [X[y== i].mean(axis=0) for i in range(n_components)]\n",
    "    )\n",
    "    gmm.fit(X)\n",
    "    \n",
    "    labels = gmm.predict(X)\n",
    "    means = gmm.means_\n",
    "    covariances = gmm.covariances_\n",
    "    \n",
    "    if not savefig:\n",
    "        return gmm\n",
    "    else:\n",
    "        # Define colors for the true labels\n",
    "        colors = [\"navy\", \"tomato\", \"darkseagreen\"]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Plot the data points with different colors for each cluster\n",
    "        for i in range(n_components):\n",
    "            cluster_data = X[labels == i]\n",
    "            plt.scatter(cluster_data[:, 0], cluster_data[:, 1], c=colors[i], label=f'Cluster {i + 1}')\n",
    "\n",
    "        # Plot ellipses to represent cluster covariances\n",
    "        for i in range(n_components):\n",
    "            covariance_matrix = covariances[i][:2, :2]\n",
    "            eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "            angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "            width, height = 2 * np.sqrt(5.991 * eigenvalues)  # 5.991 corresponds to 95% confidence interval\n",
    "\n",
    "            ellipse = Ellipse(xy=means[i], width=width, height=height, angle=angle, color=colors[i], alpha=0.2)\n",
    "            plt.gca().add_patch(ellipse)\n",
    "\n",
    "        # Plot cluster centers\n",
    "        plt.scatter(means[:, 0], means[:, 1], c='k', marker='x', s=100)\n",
    "\n",
    "        plt.xlabel(data.feature_names[0])\n",
    "        plt.ylabel(data.feature_names[1])\n",
    "        plt.title(f'Gaussian Mixture Model Clusters for {title}')\n",
    "        if savefig:\n",
    "            plt.savefig(fname=f'Gaussian_Mixture_Model_{title}')\n",
    "        plt.clf()\n",
    "        \n",
    "        # Get accuracy score\n",
    "        accuracy = accuracy_score(y, labels)\n",
    "        print(f\"{title}, GaussianMixture accuracy score: {accuracy}\")\n",
    "        return gmm\n",
    "\n",
    "def agglomerative_clustering(X, y, data, title, savefig=True):\n",
    "    # Source: https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
    "    def plot_dendrogram(model, **kwargs):\n",
    "        # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "        # create the counts of samples under each node\n",
    "        counts = np.zeros(model.children_.shape[0])\n",
    "        n_samples = len(model.labels_)\n",
    "        for i, merge in enumerate(model.children_):\n",
    "            current_count = 0\n",
    "            for child_idx in merge:\n",
    "                if child_idx < n_samples:\n",
    "                    current_count += 1  # leaf node\n",
    "                else:\n",
    "                    current_count += counts[child_idx - n_samples]\n",
    "            counts[i] = current_count\n",
    "\n",
    "        linkage_matrix = np.column_stack(\n",
    "            [model.children_, model.distances_, counts]\n",
    "        ).astype(float)\n",
    "\n",
    "        # Plot the corresponding dendrogram\n",
    "        dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "    n_clusters = len(np.unique(y))\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters, compute_distances=True, linkage='average')\n",
    "    model.fit(X)\n",
    "    if not savefig:\n",
    "        return model\n",
    "    else:\n",
    "            \n",
    "        # Define colors for the true labels\n",
    "        colors = [\"navy\", \"tomato\", \"darkseagreen\"]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Plot the dendrogram\n",
    "        plot_dendrogram(model, truncate_mode='level', p=5)\n",
    "        plt.title(f'Hierarchial Clustering Diagram for {title}')\n",
    "        plt.savefig(fname=f'Agglomerative_Clustering_{title}')\n",
    "        plt.clf()\n",
    "\n",
    "        # Get accuracy score\n",
    "        # Evaluate the clustering using ARI\n",
    "        ari = adjusted_rand_score(y, model.labels_)\n",
    "        print(f'Adjusted Rand Index (ARI): {ari}')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "c9c8df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part 2: Dimensionality Reduction Algorithms\n",
    "'''\n",
    "\n",
    "def calculate_VIF(X, transformed_X, data, algorithm):\n",
    "    df = pd.DataFrame(X, columns=data.feature_names)\n",
    "    \n",
    "    # VIF of original X\n",
    "    vif_og = pd.DataFrame()\n",
    "    vif_og[\"Feature\"] = df.columns\n",
    "    vif_og[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "\n",
    "    # Step 3: Calculate VIF after PCA\n",
    "    # Create a DataFrame from the principal components\n",
    "    df_after = pd.DataFrame(transformed_X)\n",
    "\n",
    "    # Calculate VIF for each principal component\n",
    "    vif_after = pd.DataFrame()\n",
    "    vif_after[\"Component/Projection\"] = df_after.columns\n",
    "    vif_after[\"VIF\"] = [variance_inflation_factor(df_after.values, i) for i in range(df_after.shape[1])]\n",
    "\n",
    "    # Display VIF results\n",
    "    print(f\"VIF before {algorithm}:\")\n",
    "    display(vif_og)\n",
    "\n",
    "    print(f\"VIF after {algorithm}:\")\n",
    "    display(vif_after)\n",
    "                                                                        \n",
    "def pca_reduction(X, title, data, savefig=True):\n",
    "    pca = PCA(n_components=0.95)\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std_dev = np.std(X, axis=0)\n",
    "    X_standardized = (X - mean) / std_dev\n",
    "    transformed_X = pca.fit_transform(X_standardized)\n",
    "    if not savefig:\n",
    "        return transformed_X\n",
    "    else:\n",
    "        # VIF Analysis\n",
    "        print(f'{title} VIF Analysis')\n",
    "        print('==========================')\n",
    "        calculate_VIF(X, transformed_X, data, \"PCA\")\n",
    "        # Plot the explained variance ratio as a scree plot\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, label='Explained Variance Ratio')\n",
    "        plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='r', label='Cumulative Variance')\n",
    "        plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Principal Component')\n",
    "        plt.ylabel('Variance Explained')\n",
    "        plt.title(f'Scree Plot for PCA for {title}')\n",
    "        plt.legend()\n",
    "        plt.savefig(fname=f'PCA_Scree_{title}')\n",
    "        plt.clf()\n",
    "\n",
    "        # Get the eigenvalues\n",
    "        eigenvalues = pca.explained_variance_\n",
    "        #print(eigenvalues)\n",
    "        # Plot the eigenvalue distribution\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(range(1, len(eigenvalues) + 1), eigenvalues, color='dodgerblue', alpha=0.7)\n",
    "        plt.xlabel('Eigenvalues')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.title(f'Eigenvalue Distribution in PCA for {title}')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(fname=f'PCA_Eigenvalue_Distribution_{title}')\n",
    "        plt.clf()\n",
    "\n",
    "        # Get important features\n",
    "\n",
    "        # Source: https://stackoverflow.com/a/50845697/22862849\n",
    "        # number of components\n",
    "        n_pcs = pca.components_.shape[0]\n",
    "\n",
    "        # get the index of the most important feature on EACH component\n",
    "        most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "        initial_feature_names = data.feature_names\n",
    "        # get the names\n",
    "        most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "        dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "        # build the dataframe\n",
    "        df = pd.DataFrame(dic.items())\n",
    "        display(df)\n",
    "        return transformed_X\n",
    "\n",
    "def ica_reduction(X, title, data, savefig=True):\n",
    "    ica = FastICA()\n",
    "    X_ica = ica.fit_transform(X)\n",
    "    if not savefig:\n",
    "        return X_ica\n",
    "    else:\n",
    "        # VIF Analysis\n",
    "        print(f'{title} VIF Analysis')\n",
    "        print('==========================')\n",
    "        calculate_VIF(X, X_ica, data, \"ICA\")\n",
    "\n",
    "        # Calculate feature loadings\n",
    "        feature_loadings = ica.mixing_  # Mixing matrix\n",
    "\n",
    "        # Plot the feature loadings\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        plt.imshow(np.abs(feature_loadings), cmap='plasma', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.xticks(range(X.shape[1]), data.feature_names, rotation=45)\n",
    "        plt.yticks(range(X_ica.shape[1]), [f'IC {i + 1}' for i in range(X_ica.shape[1])])\n",
    "        plt.title('Feature Loadings (Mixing Matrix)')\n",
    "        plt.savefig(fname=f'ICA_Mixing_Matrix_{title}')\n",
    "        plt.clf()\n",
    "\n",
    "        # Calculate kurtosis\n",
    "        kurtosis_values = kurtosis(X_ica, axis=0)\n",
    "        # Plot the kurtosis values of each independent component\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(range(1, len(kurtosis_values) + 1), kurtosis_values, color='dodgerblue', alpha=0.7)\n",
    "        plt.xlabel('Independent Component')\n",
    "        plt.ylabel('Kurtosis Value')\n",
    "        plt.title(f'Kurtosis Values for {title}')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(fname=f'ICA_Kurtosis_{title}')\n",
    "        plt.clf()\n",
    "        return X_ica\n",
    "\n",
    "def random_projection(X, title, data, savefig=True):\n",
    "    projection = GaussianRandomProjection(n_components=2)\n",
    "    X_rp = projection.fit_transform(X)\n",
    "    if not savefig:\n",
    "        return X_rp\n",
    "    else:\n",
    "        # VIF Analysis\n",
    "        print(f'{title} VIF Analysis')\n",
    "        print('==========================')\n",
    "        calculate_VIF(X, X_rp, data, \"Random Projection\")\n",
    "\n",
    "        # Specify the range of n_components to test\n",
    "        n_components_range = range(1, X.shape[1] + 1)\n",
    "        rmses = []\n",
    "        # Calculate the explained variance for each n_components value\n",
    "        for n_components in n_components_range:\n",
    "            projection = GaussianRandomProjection(n_components=n_components)\n",
    "            X_rp = projection.fit_transform(X)\n",
    "            X_reconstructed = np.dot(X_rp, projection.components_)\n",
    "            mse = mean_squared_error(X, X_reconstructed)\n",
    "            rmses.append(np.sqrt(mse))\n",
    "\n",
    "        # Plot the scree plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(n_components_range, rmses)\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Root Mean Squared Error')\n",
    "        plt.title(f'RMSE between Reconstructed Data and Original Data for {title}')\n",
    "        plt.grid(True)\n",
    "        plt.xticks(n_components_range)\n",
    "        plt.savefig(f\"RP_RMSE_Reconstructed_{title}\")\n",
    "        plt.clf()\n",
    "        return X_rp\n",
    "\n",
    "def mds_reduction(X, title, data, savefig=True):\n",
    "    mds = MDS()\n",
    "    X_mds = mds.fit_transform(X)\n",
    "    if not savefig:\n",
    "        return X_mds\n",
    "    else:\n",
    "        # VIF Analysis\n",
    "        print(f'{title} VIF Analysis')\n",
    "        print('==========================')\n",
    "        calculate_VIF(X, X_mds, data, \"MDS\")\n",
    "\n",
    "        # Create a scatter plot of the MDS results\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        colors = ['navy', 'turquoise', 'darkorange']\n",
    "        lw = 2\n",
    "        y = data.target\n",
    "        target_names = data.target_names\n",
    "        for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "            plt.scatter(X_mds[y == i, 0], X_mds[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                        label=target_name)\n",
    "        plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "        plt.title(f'MDS of {title}')\n",
    "        plt.savefig(fname=f'MDS_{title}')\n",
    "        plt.clf()\n",
    "        return X_mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "b577303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part 3: Run both clustering algorithms on the transformed X's using PCA, ICA, RP, and MDS\n",
    "'''\n",
    "def cluster_on_reduced_data():\n",
    "    X_list = [iris_X, breast_cancer_X]\n",
    "    data_list = [iris, breast_cancer]\n",
    "    transformed_X = {\n",
    "        'PCA': [],\n",
    "        'ICA': [],\n",
    "        'RP': [],\n",
    "        'MDS': [],\n",
    "    }\n",
    "    \n",
    "    # Get the transformed X\n",
    "    for X, data in zip(X_list, data_list):\n",
    "        transformed_X['PCA'].append(pca_reduction(X, \"\", data, savefig=False))\n",
    "        transformed_X['ICA'].append(ica_reduction(X, \"\", data, savefig=False))\n",
    "        transformed_X['RP'].append(random_projection(X, \"\", data, savefig=False))\n",
    "        transformed_X['MDS'].append(mds_reduction(X, \"\", data, savefig=False))\n",
    "    \n",
    "    for algo, new_X in transformed_X.items():\n",
    "        new_iris = new_X[0]\n",
    "        new_breast_cancer = new_X[1]\n",
    "        gaussian_clustering(new_iris, iris_Y, iris, f'Gaussian_Clustering_On_{algo}-Iris')\n",
    "        agglomerative_clustering(new_breast_cancer, breast_cancer_Y, iris, f'Agglomerative_Clustering_On_{algo}-Breast_Cancer')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "2c3d1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part 4: Run MLPClassifier with the transformed data for Iris dataset\n",
    "'''\n",
    "def classification_model(model, train_x, train_y, title, fname):\n",
    "    # Generate the learning curve data points\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, train_x, train_y, cv=5, scoring='accuracy')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(fname=fname)\n",
    "    plt.clf()\n",
    "    print('Training Scores:')\n",
    "    print(train_scores_mean)\n",
    "    print('CV Scores:')\n",
    "    print(test_scores_mean)\n",
    "    return\n",
    "\n",
    "def NN_on_reduced_data():\n",
    "    transformed_X = {\n",
    "        'PCA': [],\n",
    "        'ICA': [],\n",
    "        'RP': [],\n",
    "        'MDS': [],\n",
    "    }\n",
    "    y = iris_Y\n",
    "    transformed_X['PCA'].append(pca_reduction(iris_X, \"\", iris, savefig=False))\n",
    "    transformed_X['ICA'].append(ica_reduction(iris_X, \"\", iris, savefig=False))\n",
    "    transformed_X['RP'].append(random_projection(iris_X, \"\", iris, savefig=False))\n",
    "    transformed_X['MDS'].append(mds_reduction(iris_X, \"\", iris, savefig=False))\n",
    "    \n",
    "    NN = MLPClassifier(max_iter=1500, alpha=0.0001, hidden_layer_sizes=(50, 50), solver='lbfgs', \\\n",
    "                       learning_rate='constant', early_stopping=True)\n",
    "    for algo, new_X in transformed_X.items():\n",
    "        X = new_X[0]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        print(f\"Scores for {algo}\")\n",
    "        classification_model(NN, X_train, y_train, f'MLPClassifier {algo}-Data Learning Curve',f'MLPClassifier_With_{algo}-Iris_Data')\n",
    "        NN.fit(X_train, y_train)\n",
    "        y_pred = NN.predict(X_test)\n",
    "        print(f'Accuracy on test set: {accuracy_score(y_test, y_pred)}')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "1aed0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Part 5: NN using Clusters as new features\n",
    "'''\n",
    "def NN_w_cluster_data():\n",
    "    # Gaussian mixture labels\n",
    "    gmm = gaussian_clustering(iris_X, iris_Y, iris, \"\", savefig=False)\n",
    "    labels = np.array(gmm.predict(iris_X)).reshape((150, 1))\n",
    "    gmm_X = iris_X.copy()\n",
    "    gmm_X = np.append(gmm_X, labels, axis=1)\n",
    "\n",
    "\n",
    "    # Agglomerative clustering labels\n",
    "    agg = agglomerative_clustering(iris_X, iris_Y, iris, '', savefig=False)\n",
    "    labels = agg.labels_\n",
    "    mapping = {\n",
    "        1: 0,\n",
    "        0: 1,\n",
    "        2: 2,\n",
    "    }\n",
    "    corrected_labels = np.array([mapping[label] for label in labels]).reshape(150, 1)\n",
    "    agg_X = iris_X.copy()\n",
    "    agg_X = np.append(agg_X, corrected_labels, axis=1)\n",
    "    \n",
    "    # Init model\n",
    "    NN = MLPClassifier(max_iter=1500, alpha=0.0001, hidden_layer_sizes=(50, 50), solver='lbfgs', \\\n",
    "                       learning_rate='constant', early_stopping=True)\n",
    "    print('Training w/ GMM cluster data')\n",
    "    # Train on GMM data\n",
    "    gmm_X_train, gmm_X_test, gmm_y_train, gmm_y_test = train_test_split(gmm_X, iris_Y, test_size=0.2)\n",
    "    classification_model(NN, gmm_X_train, gmm_y_train, f'MLPClassifier GMM Cluster Learning Curve', \\\n",
    "                        f'MLPClassifier_With_GMM_Cluster_Iris_Data')\n",
    "    NN.fit(gmm_X_train, gmm_y_train)\n",
    "    y_pred = NN.predict(gmm_X_test)\n",
    "    print(f'Accuracy on test set: {accuracy_score(gmm_y_test, y_pred)}')\n",
    "    \n",
    "    print('=================================')\n",
    "    print('Training w/ AGG cluster data')\n",
    "    # Train on AgglomerativeClustering data\n",
    "    agg_X_train, agg_X_test, agg_y_train, agg_y_test = train_test_split(agg_X, iris_Y, test_size=0.2)\n",
    "    classification_model(NN, agg_X_train, agg_y_train, f'MLPClassifier AGG Cluster Learning Curve', \\\n",
    "                        f'MLPClassifier_With_AGG_Cluster_Iris_Data')\n",
    "    NN.fit(agg_X_train, agg_y_train)\n",
    "    y_pred = NN.predict(agg_X_test)\n",
    "    print(f'Accuracy on test set: {accuracy_score(agg_y_test, y_pred)}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "a07f09e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training w/ GMM cluster data\n",
      "Training Scores:\n",
      "[1.         1.         0.98461538 0.98378378 0.9875    ]\n",
      "CV Scores:\n",
      "[0.89166667 0.93333333 0.90833333 0.96666667 0.96666667]\n",
      "Accuracy on test set: 0.9666666666666667\n",
      "=================================\n",
      "Training w/ AGG cluster data\n",
      "Training Scores:\n",
      "[1.         1.         1.         0.99189189 0.98541667]\n",
      "CV Scores:\n",
      "[0.76666667 0.91666667 0.95       0.95       0.96666667]\n",
      "Accuracy on test set: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #gaussian_clustering(breast_cancer_X, breast_cancer_Y, breast_cancer, \"Breast_Cancer_Dataset\")\n",
    "    #gaussian_clustering(iris_X, iris_Y, iris, \"Iris_Dataset\")\n",
    "    #agglomerative_clustering(breast_cancer_X, breast_cancer_Y, breast_cancer, \"Breast_Cancer_Dataset\")\n",
    "    #agglomerative_clustering(iris_X, iris_Y, iris, \"Iris_Dataset\")\n",
    "    #pca_reduction(breast_cancer_X, 'Breast_Cancer_Dataset', breast_cancer)\n",
    "    #pca_reduction(iris_X, 'Iris_Dataset', iris)\n",
    "    #ica_reduction(iris_X, 'Iris_Dataset', iris)\n",
    "    #ica_reduction(breast_cancer_X, 'Breast_Cancer_Dataset', breast_cancer)\n",
    "    #random_projection(iris_X, 'Iris_Dataset', iris)\n",
    "    #random_projection(breast_cancer_X, 'Breast_Cancer_Dataset', breast_cancer)\n",
    "    #mds_reduction(iris_X, 'Iris_Dataset', iris)\n",
    "    #mds_reduction(breast_cancer_X, 'Breast_Cancer_Dataset', breast_cancer)\n",
    "    #cluster_on_reduced_data()\n",
    "    #NN_on_reduced_data()\n",
    "    NN_w_cluster_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
